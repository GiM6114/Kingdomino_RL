Compare heuristic methods that tries to increase immediate rewards + maximize sizes of connected components + minimize number of connected components
Making them play against each other is good because they don't learn to focus on one type of tiles, bcs if they do the strategy becomes bad

TODO: 
* conditional (on action, previous tile, current tiles) transformer + positional encoding in the image
* add prioritized experience replay
* add double q learning
* add reward normalization
* retry first representation (with board rep then action)

A board just before network:
(channels, board_size, board_size) = (8,9,9)
After first layer (c=32, kernel_size=3, stride=1) :
32*9*9


New architecture (simple version):

auxiliary_info: previous_tile, current_tiles
Class of network called PlayerNetwork:
- Input: previous tile, 